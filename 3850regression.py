# -*- coding: utf-8 -*-
"""3850Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xJkjdJKlvY9FOciyDbiVO5WqG7a2255O
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Drop NA Version
# dt = pd.read_excel('/content/dropNa_USE_THIS_FINAL.xlsx',index_col = 'OBJECTID')
dt = pd.read_excel('/content/dropNa_USE_THIS_FINAL.xlsx')

# ORG Ver.
# dt = pd.read_excel('/content/USE_THIS_for_Regression_New.xlsx')
#

dt.head()

list(dt.columns)

y_index = ['Temp_Mean','Mean greenview', 'Tree_density',
 'Mean PM 2.5',
 'Temp_Mean',
 'Mean O3',
 'Mean NO',
 'Mean NO2']

#impute missing data in y columns?
from sklearn.impute import SimpleImputer
#imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')

x_index = ['Mean greenview','Pct White','Pct Bachelors or More','Pct Unemployed','Ln Income','Ln Housing Value','Pct Rent 50 or More','Population Density']
x = dt[x_index].values
x

#=============================================#
# Simple Multiple Linear Regression [Feature Scaling?]
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
for ele in y_index:
  
  #process x and y, as there are NAN values in y, and we dont want to drop them all  
  x_for_y = dt.copy()
  x_for_y = x_for_y.dropna(subset=[ele,'Mean greenview','Pct White','Pct Bachelors or More','Pct Unemployed','Ln Income','Ln Housing Value','Pct Rent 50 or More','Population Density'])
  y = x_for_y[ele].values
  x = x_for_y[x_index].values
  #x = x[~np.isnan(x).any(axis=1)]

  #print(np.argwhere(np.isnan(x)))
  #print(np.argwhere(np.isnan(y)))
  #print(x.shape)
  print(f'Obs = {y.shape}')


  regressor.fit(x,y)
  print(f'Current Y: {ele}')
  cof_count = 0
  for cofs in regressor.coef_:
    print(f'Coef {x_index[cof_count]} = {cofs}')
    cof_count += 1
  print(f'Constant = {regressor.intercept_}')
  print(f'R_score = {regressor.score(x,y)}')
  print('\n')

#=============================================#
# Simple Multiple Linear Regression [Feature Scaling?]
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
for ele in y_index:
  
  #process x and y, as there are NAN values in y, and we dont want to drop them all  
  x_for_y = dt.copy()
  x_for_y = x_for_y.dropna(subset=[ele,'Mean greenview','Pct White','Pct Bachelors or More','Pct Unemployed','Ln Income','Ln Housing Value','Pct Rent 50 or More','Population Density'])
  y = x_for_y[ele].values
  x = x_for_y[x_index].values
  #x = x[~np.isnan(x).any(axis=1)]

  #print(np.argwhere(np.isnan(x)))
  #print(np.argwhere(np.isnan(y)))
  #print(x.shape)
  print(f'Obs = {y.shape}')


  regressor.fit(x,y)
  print(f'Current Y: {ele}')
  cof_count = 0
  for cofs in regressor.coef_:
    print(f'Coef {x_index[cof_count]} = {cofs}')
    cof_count += 1
  print(f'Constant = {regressor.intercept_}')
  print(f'R_score = {regressor.score(x,y)}')
  print('\n')
  N = int(y.shape[0])
  p = 9  # plus one because LinearRegression adds an intercept term

  x_with_intercept = np.empty(shape=(N, p), dtype=np.float)
  x_with_intercept[:, 0] = 1
  x_with_intercept[:, 1:p] = x

  ols = sm.OLS(y, x_with_intercept)
  ols_result = ols.fit()
  result = ols_result.summary()
  print(result)
  print('\n')
  print('+=====================================+')